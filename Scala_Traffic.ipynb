{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "several-celebration",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-horizon",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.rdd.RDD\n",
    "\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.expressions._\n",
    "import org.apache.spark.sql.Row\n",
    "import spark.implicits._\n",
    "import scala.collection.mutable._\n",
    "import org.apache.spark.graphx.lib._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elegant-genre",
   "metadata": {},
   "source": [
    "## Loading DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "miniature-budapest",
   "metadata": {},
   "outputs": [],
   "source": [
    "// File location and type\n",
    "val file_location = \"violationtraffic.csv\"\n",
    "val file_type = \"csv\"\n",
    "\n",
    "// CSV options\n",
    "val infer_schema = \"True\"\n",
    "val first_row_is_header = \"True\"\n",
    "val delimiter = \",\"\n",
    "\n",
    "// The applied options are for CSV files. For other file types, these will be ignored.\n",
    "val df = spark.read.format(file_type).option(\"inferSchema\", infer_schema).option(\"header\", first_row_is_header).option(\"sep\", delimiter).load(file_location)\n",
    "\n",
    "\n",
    "df.limit(20).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complimentary-mechanism",
   "metadata": {},
   "source": [
    "## Euclidean-LSH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revolutionary-harmony",
   "metadata": {},
   "source": [
    "<b> Hash Function (Random Hyperplanes) </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-nelson",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.linalg.SparseVector\n",
    "import scala.collection.mutable.ArrayBuffer\n",
    "import scala.util.Random\n",
    "\n",
    "/**\n",
    " * Simple hashing function implements random hyperplane based hash functions described in\n",
    " * http://www.cs.princeton.edu/courses/archive/spring04/cos598B/bib/CharikarEstim.pdf\n",
    " * r is a random vector. Hash function h_r(u) operates as follows:\n",
    " * if r.u < 0 //dot product of two vectors\n",
    " *    h_r(u) = 0\n",
    " *  else\n",
    " *    h_r(u) = 1\n",
    " */\n",
    "class Hasher(val r: Array[Double]) extends Serializable {\n",
    "\n",
    "  /** hash SparseVector v with random vector r */\n",
    "  def hash(u : SparseVector) : Int = {\n",
    "    val rVec: Array[Double] = u.indices.map(i => r(i))\n",
    "    val hashVal = (rVec zip u.values).map(_tuple => _tuple._1 * _tuple._2).sum\n",
    "    if (hashVal > 0) 1 else 0\n",
    "  }\n",
    "\n",
    "}\n",
    "\n",
    "object Hasher {\n",
    "\n",
    "  /** create a new instance providing size of the random vector Array [Double] */\n",
    "  def apply (size: Int, seed: Long = System.nanoTime) = new Hasher(r(size, seed))\n",
    "\n",
    "  /** create a random vector whose whose components are -1 and +1 */\n",
    "  def r(size: Int, seed: Long) : Array[Double] = {\n",
    "    val buf = new ArrayBuffer[Double]\n",
    "    val rnd = new Random(seed)\n",
    "    for (_ <- 0 until size)\n",
    "      buf += (if (rnd.nextGaussian() < 0) -1 else 1)\n",
    "    buf.toArray\n",
    "  }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electronic-eligibility",
   "metadata": {},
   "source": [
    "<b> Function to save and Load Lsh Model </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greenhouse-canada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.hadoop.fs.Path\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.mllib.linalg.SparseVector\n",
    "import org.apache.spark.rdd.RDD\n",
    "import scala.collection.mutable.ListBuffer\n",
    "import org.apache.spark.mllib.util.Saveable\n",
    "\n",
    "import org.json4s._\n",
    "import org.json4s.JsonDSL._\n",
    "import org.json4s.jackson.JsonMethods._\n",
    "\n",
    "/** Helper functions for save/load data from mllib package.\n",
    "  * TODO: Remove and use Loader functions from mllib. */\n",
    "object Loader {\n",
    "\n",
    "  /** Returns URI for path/data using the Hadoop filesystem */\n",
    "  def dataPath(path: String): String = new Path(path, \"data\").toUri.toString\n",
    "\n",
    "  /** Returns URI for path/metadata using the Hadoop filesystem */\n",
    "  def metadataPath(path: String): String = new Path(path, \"metadata\").toUri.toString\n",
    "\n",
    "  /** Returns URI for path/metadata using the Hadoop filesystem */\n",
    "  def hasherPath(path: String): String = new Path(path, \"hasher\").toUri.toString\n",
    "\n",
    "  /**\n",
    "   * Load metadata from the given path.\n",
    "   * @return (class name, version, metadata)\n",
    "   */\n",
    "  def loadMetadata(sc: SparkContext, path: String): (String, String, JValue) = {\n",
    "    implicit val formats: DefaultFormats.type = DefaultFormats\n",
    "    val metadata = parse(sc.textFile(metadataPath(path)).first())\n",
    "    val clazz = (metadata \\ \"class\").extract[String]\n",
    "    val version = (metadata \\ \"version\").extract[String]\n",
    "    (clazz, version, metadata)\n",
    "  }\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "happy-association",
   "metadata": {},
   "source": [
    "<b> Main LSHModel Class And Object </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-watts",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.json4s._\n",
    "import org.json4s.JsonDSL._\n",
    "import org.json4s.jackson.JsonMethods._\n",
    "\n",
    "\n",
    "\n",
    "/** Create LSH model for maximum m number of elements in each vector.\n",
    "  *\n",
    "  * @param m max number of possible elements in a vector\n",
    "  * @param numHashFunc number of hash functions\n",
    "  * @param numHashTables number of hashTables.\n",
    "  *\n",
    "  * */\n",
    "class LSHModel(val m: Int, val numHashFunc : Int, val numHashTables: Int)\n",
    "  extends Serializable with Saveable {\n",
    "\n",
    "  /** generate numHashFunc * numBands randomly generated hash functions and store them in hashFunctions */\n",
    "  private val _hashFunctions = ListBuffer[Hasher]()\n",
    "  for (_ <- 0 until numHashFunc * numHashTables)\n",
    "    _hashFunctions += Hasher(m)\n",
    "  final var hashFunctions: List[(Hasher, Int)] = _hashFunctions.toList.zipWithIndex\n",
    "\n",
    "  /** the \"hashTables\" ((hashTableID, hash key), vector_id) */\n",
    "  var hashTables: RDD[((Int, String), Long)] = _\n",
    "\n",
    "  /** generic filter function for hashTables. */\n",
    "  def filter(f: (((Int, String), Long)) => Boolean): RDD[((Int, String), Long)] =\n",
    "    hashTables.map(a => a).filter(f)\n",
    "\n",
    "  /** hash a single vector against an existing model and return the candidate buckets */\n",
    "  def filter(data: SparseVector, model: LSHModel, itemID: Long): RDD[Long] = {\n",
    "    val hashKey = hashFunctions.map(h => h._1.hash(data)).mkString(\"\")\n",
    "    hashTables.filter(x => x._1._2 == hashKey).map(a => a._2)\n",
    "  }\n",
    "\n",
    "  /** creates hashValue for each hashTable.*/\n",
    "  def hashValue(data: SparseVector): List[(Int, String)] =\n",
    "    hashFunctions.map(a => (a._2 % numHashTables, a._1.hash(data)))\n",
    "    .groupBy(_._1)\n",
    "    .map(x => (x._1, x._2.map(_._2).mkString(\"\"))).toList\n",
    "\n",
    "  /** returns candidate set for given vector id.*/\n",
    "  def getCandidates(vId: Long): RDD[Long] = {\n",
    "    val buckets = hashTables.filter(x => x._2 == vId).map(x => x._1).distinct().collect()\n",
    "    hashTables.filter(x => buckets contains x._1).map(x => x._2).filter(x => x != vId)\n",
    "  }\n",
    "\n",
    "  /** returns candidate set for given vector.*/\n",
    "  def getCandidates(v: SparseVector): RDD[Long] = {\n",
    "    val hashVal = hashValue(v)\n",
    "    hashTables.filter(x => hashVal contains x._1).map(x => x._2)\n",
    "  }\n",
    "\n",
    "  /** adds a new sparse vector with vector Id: vId to the model. */\n",
    "  def add (vId: Long, v: SparseVector, sc: SparkContext): LSHModel = {\n",
    "    val newRDD = sc.parallelize(hashValue(v).map(a => (a, vId)))\n",
    "    hashTables ++ newRDD\n",
    "    this\n",
    "  }\n",
    "\n",
    "  /** remove sparse vector with vector Id: vId from the model. */\n",
    "  def remove (vId: Long, sc: SparkContext): LSHModel = {\n",
    "    hashTables =  hashTables.filter(x => x._2 != vId)\n",
    "    this\n",
    "  }\n",
    "\n",
    "      \n",
    "  override def save(sc: SparkContext, path: String): Unit = LSHModel.SaveLoadV0_0_1.save(sc, this, path)\n",
    "\n",
    "  def formatVersion: String = \"0.0.1\"\n",
    "\n",
    "}\n",
    "\n",
    "object LSHModel {\n",
    "\n",
    "  def load(sc: SparkContext, path: String): LSHModel = {\n",
    "    LSHModel.SaveLoadV0_0_1.load(sc, path)\n",
    "  }\n",
    "\n",
    "  private object SaveLoadV0_0_1 {\n",
    "\n",
    "    private val thisFormatVersion = \"0.0.1\"\n",
    "    private val thisClassName = this.getClass.getName\n",
    "\n",
    "    def save(sc: SparkContext, model: LSHModel, path: String): Unit = {\n",
    "\n",
    "      val metadata = compact(render((\"class\" -> thisClassName) ~ (\"version\" -> thisFormatVersion)))\n",
    "\n",
    "      //save metadata info\n",
    "      sc.parallelize(Seq(metadata), 1).saveAsTextFile(Loader.metadataPath(path))\n",
    "\n",
    "      //save hash functions as (hashTableId, randomVector)\n",
    "      sc.parallelize(model.hashFunctions.map(x => (x._2, x._1.r.mkString(\",\"))).map(_.productIterator.mkString(\",\"))).saveAsTextFile(Loader.hasherPath(path))\n",
    "\n",
    "     //save data as (hashTableId#, hashValue, vectorId)\n",
    "      model.hashTables.map(x => (x._1._1, x._1._2, x._2)).map(_.productIterator.mkString(\",\")).saveAsTextFile(Loader.dataPath(path))\n",
    "\n",
    "    }\n",
    "\n",
    "    def load(sc: SparkContext, path: String): LSHModel = {\n",
    "\n",
    "      implicit val formats: DefaultFormats.type = DefaultFormats\n",
    "      val (className, formatVersion, _) = Loader.loadMetadata(sc, path)\n",
    "      assert(className == thisClassName)\n",
    "      assert(formatVersion == thisFormatVersion)\n",
    "      val hashTables = sc.textFile(Loader.dataPath(path)).map(x => x.split(\",\")).map(x => ((x(0).toInt, x(1)), x(2).toLong))\n",
    "      val hashers = sc.textFile(Loader.hasherPath(path)).map(a => a.split(\",\")).map(x => (x.head, x.tail)).map(x => (new Hasher(x._2.map(_.toDouble)), x._1.toInt)).collect().toList\n",
    "      val numBands = hashTables.map(x => x._1._1).distinct.count()\n",
    "      val numHashFunc = hashers.size / numBands\n",
    "\n",
    "      //Validate loaded data\n",
    "      //check size of data\n",
    "      assert(hashTables.count != 0, s\"Loaded hashTable data is empty\")\n",
    "      //check size of hash functions\n",
    "      assert(hashers.nonEmpty, s\"Loaded hasher data is empty\")\n",
    "      //check hashValue size. Should be equal to numHashFunc\n",
    "      assert(hashTables.map(x => x._1._2).filter(x => x.length != numHashFunc).collect().length == 0,s\"hashValues in data does not match with hash functions\")\n",
    "\n",
    "      //create model\n",
    "      val model = new LSHModel(0, numHashFunc.toInt, numBands.toInt)\n",
    "      model.hashFunctions = hashers\n",
    "      model.hashTables = hashTables\n",
    "\n",
    "      model\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "class LSH(data : RDD[(Long, SparseVector)] = null, m: Int = 0, numHashFunc : Int = 4, numHashTables : Int = 4) extends Serializable {\n",
    "\n",
    "  def run() : LSHModel = {\n",
    "\n",
    "    //create a new model object\n",
    "    val model = new LSHModel(m, numHashFunc, numHashTables)\n",
    "\n",
    "    val dataRDD = data.cache()\n",
    "\n",
    "    //compute hash keys for each vector\n",
    "    // - hash each vector numHashFunc times\n",
    "    // - concat each hash value to create a hash key\n",
    "    // - position hashTable id hash keys and vector id into a new RDD.\n",
    "    // - creates RDD of ((hashTable#, hash_key), vec_id) tuples.\n",
    "    model.hashTables = dataRDD.map(v => (model.hashFunctions.map(h => (h._1.hash(v._2), h._2 % numHashTables)), v._1)).map(x => x._1.map(a => ((a._2, x._2), a._1))).flatMap(a => a).groupByKey().map(x => ((x._1._1, x._2.mkString(\"\")), x._1._2)).cache()\n",
    "\n",
    "    model\n",
    "\n",
    "  }\n",
    "\n",
    "  def cosine(a: SparseVector, b: SparseVector): Double = {\n",
    "    val intersection = a.indices.intersect(b.indices)\n",
    "    val magnitudeA = intersection.map(x => Math.pow(a.apply(x), 2)).sum\n",
    "    val magnitudeB = intersection.map(x => Math.pow(b.apply(x), 2)).sum\n",
    "    intersection.map(x => a.apply(x) * b.apply(x)).sum / (Math.sqrt(magnitudeA) * Math.sqrt(magnitudeB))\n",
    "  }\n",
    "\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-florence",
   "metadata": {},
   "source": [
    "<b> Creating (time, car, cameraId) Df </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "republican-vocabulary",
   "metadata": {},
   "outputs": [],
   "source": [
    "var basket_holder = df.select(col(\"PassDatetime\").alias(\"time\"), col(\"MasterPlateNumber\").alias(\"car\"), col(\"DeviceId\").alias(\"camera\"))\n",
    ".withColumn(\"time\", dayofyear(col(\"time\")))\n",
    "\n",
    "basket_holder.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linear-organic",
   "metadata": {},
   "source": [
    "<b> Create Dataframe of distinct Camera Ids </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utility-arbitration",
   "metadata": {},
   "outputs": [],
   "source": [
    "//list of distinct items\n",
    "val items_count: Int = df.select(col(\"DeviceId\")).distinct().count().toInt\n",
    "val distinct_cameras = df.select(col(\"DeviceId\").alias(\"camera\")).distinct()\n",
    "println(\"items_count: \" + items_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equivalent-pitch",
   "metadata": {},
   "source": [
    "<b> Create Hash Df to map value of cameraId </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "human-collection",
   "metadata": {},
   "outputs": [],
   "source": [
    "val distinct_cameras_hash = distinct_cameras.rdd.zipWithIndex.map(x => (x._1(0).asInstanceOf[Int], x._2.asInstanceOf[Int])).toDF(\"camera\", \"id\")\n",
    "distinct_cameras_hash.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formal-brunei",
   "metadata": {},
   "source": [
    "<b>Saving Table Of CameraId Hash </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lightweight-value",
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_cameras_hash.coalesce(1).write.format(\"csv\").option(\"header\", \"true\").save(\"camera_id.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accurate-broadcasting",
   "metadata": {},
   "source": [
    "<b> Replace CameraId with New Hash </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "postal-scientist",
   "metadata": {},
   "outputs": [],
   "source": [
    "val new_basket_holder = basket_holder.join(distinct_cameras_hash, \"camera\").drop(\"camera\")\n",
    "new_basket_holder.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attractive-professional",
   "metadata": {},
   "source": [
    "<b> First Building path of each car Then Make it sparse </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "through-guard",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "\n",
    "//row(1).asInstanceOf[Int].longValue*1000 + row(0).asInstanceOf[Int].longValue\n",
    "val hour_basket = new_basket_holder.rdd.map(row => (row(1).asInstanceOf[Int],(row(2).asInstanceOf[Int], 1.0)))\n",
    "\n",
    "\n",
    "val hour_basket_grouped = hour_basket.distinct.groupByKey()\n",
    "//hour_basket_grouped.take(3)\n",
    "//convert each car's camera list to tuple of (car_id, SparseVector_of_cameras)\n",
    "val sparseVectorData = hour_basket_grouped.map(a=>(a._1.asInstanceOf[Long], Vectors.sparse(items_count.asInstanceOf[Int], a._2.toSeq).asInstanceOf[SparseVector]))\n",
    "\n",
    "sparseVectorData.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inside-orange",
   "metadata": {},
   "source": [
    "<b> Setup LSH </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vietnamese-dover",
   "metadata": {},
   "outputs": [],
   "source": [
    "val lsh = new LSH(sparseVectorData, items_count.asInstanceOf[Int], numHashFunc = 8, numHashTables = 15)\n",
    "val model = lsh.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bound-agenda",
   "metadata": {},
   "source": [
    "<b> Show Nearest Neighbors Candidate </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signal-hometown",
   "metadata": {},
   "outputs": [],
   "source": [
    "val candList = model.getCandidates(762817122)\n",
    "println(\"Number of Candidates: \"+ candList.count())\n",
    "println(\"Candidate List: \" + candList.collect().toList)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increased-continent",
   "metadata": {},
   "source": [
    "## <b> Graphx And Community Detection </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjacent-hunger",
   "metadata": {},
   "source": [
    "<b> GroupBy Data Base On Car and DayOfYear </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-visitor",
   "metadata": {},
   "outputs": [],
   "source": [
    "val path_df = df.select(col(\"PassDatetime\").alias(\"time\"), col(\"MasterPlateNumber\").alias(\"car\"), col(\"DeviceId\").alias(\"camera\")).groupBy(col(\"car\"), dayofyear(col(\"time\"))).agg(collect_set(struct(\"time\", \"camera\")).alias(\"list_col\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governing-poison",
   "metadata": {},
   "source": [
    "<b> Sort Paths base On PassDateTime so we have a directed Path </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "martial-judge",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "val path_find = udf ((row_detail: Seq[Row]) =>  {\n",
    "    \n",
    "\n",
    "  val arr1Tup: Seq[(String, Int)] = row_detail.map{case Row(s:String,i:Int) => (s,i)}\n",
    "  val res = arr1Tup.sortBy(_._1)\n",
    "  res.map(x => x._2)\n",
    "})\n",
    "\n",
    "\n",
    "val path_df2 = path_df.withColumn(\"paths\", path_find(col(\"list_col\")))\n",
    "val path_df3 = path_df2.filter(size($\"paths\") > 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infectious-jonathan",
   "metadata": {},
   "source": [
    "<b> Building Edge Tuples base on Adjacency List </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exciting-equation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_build(row_detail: WrappedArray[Int]): Array[Tuple2[Long, Long]] = {\n",
    "\n",
    "  var res : Array[Tuple2[Long, Long]] = Array()\n",
    "    \n",
    "  for (i <- 0 until row_detail.length - 1){\n",
    "      \n",
    "     res :+= (row_detail(i).asInstanceOf[Long],row_detail(i+1).asInstanceOf[Long])\n",
    "  }\n",
    "    res\n",
    "  \n",
    "}\n",
    "\n",
    "val path_df4 = path_df3.select(col(\"car\"), col(\"paths\")).rdd.flatMap(x => path_build(x(1).asInstanceOf[WrappedArray[Int]]))//.reduceByKey(_ | _)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assured-furniture",
   "metadata": {},
   "source": [
    "<b> Build Graph base on Edge Tuples </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chubby-liberia",
   "metadata": {},
   "outputs": [],
   "source": [
    "val graph = Graph.fromEdgeTuples(path_df4, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "steady-chamber",
   "metadata": {},
   "source": [
    "<b> PageRank Algorithm </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strange-stream",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Run PageRank\n",
    "val ranks = graph.pageRank(0.0001, 0.15).vertices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "terminal-bible",
   "metadata": {},
   "source": [
    "<b> Strongly Connected Componrnts Algorithm </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demanding-adapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "val connected_components = graph.stronglyConnectedComponents(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bizarre-surgeon",
   "metadata": {},
   "source": [
    "<b> Extract Strongly Components as Array[List] </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rotary-procedure",
   "metadata": {},
   "outputs": [],
   "source": [
    "val strong_components = connected_components.vertices.map(_.swap).groupByKey.map(_._2).map(x => x.toList)\n",
    "strong_components.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-billion",
   "metadata": {},
   "source": [
    "<b> Convert Array[List] to DF so we can save it as CSV </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weird-repository",
   "metadata": {},
   "outputs": [],
   "source": [
    "var strong_components_df = spark.createDataFrame(strong_components).withColumn(\"Community\", col(\"value\")).drop(\"value\")\n",
    "\n",
    "strong_components_df = strong_components_df.withColumn(\"Community\", col(\"Community\").cast(\"String\"))\n",
    "\n",
    "println(\"some communities : \")\n",
    "strong_components_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "color-integer",
   "metadata": {},
   "source": [
    "<b> Save SCG Result as CSV </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marine-texas",
   "metadata": {},
   "outputs": [],
   "source": [
    "strong_components_df.coalesce(1).write.format(\"csv\").option(\"header\", \"true\").save(\"Strong_Community_Detection.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fancy-tonight",
   "metadata": {},
   "source": [
    "<b> LabelPropagation Algorithm </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threatened-burton",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scala.collection.{mutable, Map}\n",
    "import scala.reflect.ClassTag\n",
    "\n",
    "import org.apache.spark.graphx._\n",
    "\n",
    "/** Label Propagation algorithm. */\n",
    "object LabelPropagation {\n",
    "  /**\n",
    "   * Run static Label Propagation for detecting communities in networks.\n",
    "   *\n",
    "   * Each node in the network is initially assigned to its own community. At every superstep, nodes\n",
    "   * send their community affiliation to all neighbors and update their state to the mode community\n",
    "   * affiliation of incoming messages.\n",
    "   *\n",
    "   * LPA is a standard community detection algorithm for graphs. It is very inexpensive\n",
    "   * computationally, although (1) convergence is not guaranteed and (2) one can end up with\n",
    "   * trivial solutions (all nodes are identified into a single community).\n",
    "   *\n",
    "   * @tparam ED the edge attribute type (not used in the computation)\n",
    "   *\n",
    "   * @param graph the graph for which to compute the community affiliation\n",
    "   * @param maxSteps the number of supersteps of LPA to be performed. Because this is a static\n",
    "   * implementation, the algorithm will run for exactly this many supersteps.\n",
    "   *\n",
    "   * @return a graph with vertex attributes containing the label of community affiliation\n",
    "   */\n",
    "  def run[VD, ED: ClassTag](graph: Graph[VD, ED], maxSteps: Int): Graph[VertexId, ED] = {\n",
    "    require(maxSteps > 0, s\"Maximum of steps must be greater than 0, but got ${maxSteps}\")\n",
    "\n",
    "    val lpaGraph = graph.mapVertices { case (vid, _) => vid }\n",
    "    def sendMessage(e: EdgeTriplet[VertexId, ED]): Iterator[(VertexId, Map[VertexId, Long])] = {\n",
    "      Iterator((e.srcId, Map(e.dstAttr -> 1L)), (e.dstId, Map(e.srcAttr -> 1L)))\n",
    "    }\n",
    "    def mergeMessage(count1: Map[VertexId, Long], count2: Map[VertexId, Long])\n",
    "      : Map[VertexId, Long] = {\n",
    "      // Mimics the optimization of breakOut, not present in Scala 2.13, while working in 2.12\n",
    "      val map = mutable.Map[VertexId, Long]()\n",
    "      (count1.keySet ++ count2.keySet).foreach { i =>\n",
    "        val count1Val = count1.getOrElse(i, 0L)\n",
    "        val count2Val = count2.getOrElse(i, 0L)\n",
    "        map.put(i, count1Val + count2Val)\n",
    "      }\n",
    "      map\n",
    "    }\n",
    "    def vertexProgram(vid: VertexId, attr: Long, message: Map[VertexId, Long]): VertexId = {\n",
    "      if (message.isEmpty) attr else message.maxBy(_._2)._1\n",
    "    }\n",
    "    val initialMessage = Map[VertexId, Long]()\n",
    "    Pregel(lpaGraph, initialMessage, maxIterations = maxSteps)(\n",
    "      vprog = vertexProgram,\n",
    "      sendMsg = sendMessage,\n",
    "      mergeMsg = mergeMessage)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rapid-safety",
   "metadata": {},
   "source": [
    "<b> Run it with 5 iteration </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "registered-seminar",
   "metadata": {},
   "outputs": [],
   "source": [
    "val lp =  LabelPropagation\n",
    "val labeld_graph = lp.run(graph, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sticky-abortion",
   "metadata": {},
   "source": [
    "<b> CameraId with CommunityId Tuples </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alert-jordan",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeld_graph.vertices.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-working",
   "metadata": {},
   "source": [
    "<b> GroupBy CommunityId So we make Df </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-intermediate",
   "metadata": {},
   "outputs": [],
   "source": [
    "val communities = labeld_graph.vertices.map(x => (x._2, Array(x._1))).reduceByKey(_++_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medical-welsh",
   "metadata": {},
   "source": [
    "<b> Make Df of Communities </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alert-career",
   "metadata": {},
   "outputs": [],
   "source": [
    "var communitiesDf = communities.toDF(\"CommunityId\", \"CameraId\")\n",
    "\n",
    "communitiesDf = communitiesDf.withColumn(\"CameraId\", col(\"CameraId\").cast(\"String\"))\n",
    "\n",
    "println(\"some communities : \")\n",
    "communitiesDf.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabulous-court",
   "metadata": {},
   "source": [
    "<b> Save communities as CSV </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accomplished-dividend",
   "metadata": {},
   "outputs": [],
   "source": [
    "communitiesDf.coalesce(1).write.format(\"csv\").option(\"header\", \"true\").save(\"Community_Detection.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powerful-extreme",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
